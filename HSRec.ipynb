{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "tw_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ne vom folosi de un lemmatizer pentru a aduce cuvintele la forma din dictionar\n",
    "# vom elimina toate stop-words urile\n",
    "# pentru token-izare ne vom folosi de un tweer tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    df = pd.read_csv(filename,delimiter = ',')      #citim dataset ul \n",
    "    tweets = df['tweet'].values     \n",
    "    labels = df['label'].values\n",
    "                                                                              # functie prin care\n",
    "    shuffle_stratified = StratifiedShuffleSplit(n_splits=1, test_size=0.2)    # impartim dataset ul in 80% date de train\n",
    "                                                                              # si 20% date de test \n",
    "    for train_index, test_index in shuffle_stratified.split(tweets, labels):\n",
    "        tweets_train, tweets_test = tweets[train_index], tweets[test_index]\n",
    "        labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "\n",
    "\n",
    "        \n",
    "    return tweets_train,labels_train,tweets_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train, y_train,tweets_test,y_test = get_data(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tweets = []\n",
    "for tweet in tweets_train:\n",
    "    tokens = tw_tokenizer.tokenize(tweet)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    \n",
    "    for token in filtered_tokens:\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "    tweet = \" \".join(filtered_tokens)\n",
    "    filtered_tweets.append(tweet)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vom tokeniza fiecare tweet, eliminand stop words-urile,apoi vom lemmatiza fiecare token, si unim tweet-ul inpoi\n",
    "# insa va fi format din cuvintele prelucrate pana acum, vom avea o noua lista de tweet uri -> filtered_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(lowercase=True, analyzer='word', stop_words='english')\n",
    "\n",
    "count_vectorizer.fit(filtered_tweets)    #se creeaza dictionarul de token-uri, se mapeaza fiecare token la o pozitie\n",
    "X_train = count_vectorizer.transform(filtered_tweets)    #extragerea de token count-uri atat din train cat si din test\n",
    "X_test = count_vectorizer.transform(tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vom converti colectia noastra de tweeturi intr-o matrice de token count-uri folosind count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9596433599249179\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      5945\n",
      "           1       0.82      0.55      0.66       448\n",
      "\n",
      "    accuracy                           0.96      6393\n",
      "   macro avg       0.89      0.77      0.82      6393\n",
      "weighted avg       0.96      0.96      0.96      6393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vom folosi clasificatorul Naive Bayes \n",
    "model = MultinomialNB(alpha=0.01)    # initializarea modelului\n",
    "model.fit(X_train, y_train)          # si antrenarea acestuia folosind matricea sparsa de train\n",
    "\n",
    "predictions = model.predict(X_test)  # predictia\n",
    "\n",
    "print(accuracy_score(y_test, predictions))  # vom afisa metricile de acuratete pentru modelul nostru\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
